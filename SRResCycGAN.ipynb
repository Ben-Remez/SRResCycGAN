{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "454b53b3",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd7d1565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import ToTensor, Resize, Compose\n",
    "\n",
    "import torchvision.models as models\n",
    "from torchvision.models import vgg19\n",
    "from torchvision.transforms import Normalize\n",
    "from torch.cuda.amp import GradScaler, autocast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1a0a32",
   "metadata": {},
   "source": [
    "# Downsample HR images into LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "743803ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_image(image_path, output_path, scale_factor=4):\n",
    "    with Image.open(image_path) as img:\n",
    "        # Convert RGBA to RGB if necessary\n",
    "        if img.mode == 'RGBA':\n",
    "            img = img.convert('RGB')\n",
    "        lr_img = img.resize((img.width // scale_factor, img.height // scale_factor), Image.BICUBIC)\n",
    "        lr_img.save(output_path, 'JPEG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aedc4ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 00d85df95f49fc.jpg\n",
      "2 00f88bb445ed07.jpg\n",
      "3 06daac6f0ec143.jpg\n",
      "4 0eb65a862a018c.jpg\n",
      "5 10f8ccb1cae577.jpg\n",
      "6 11480a72b92855.jpg\n",
      "7 114bc748f3bf99.jpg\n",
      "8 16c6fd2cf1f624.jpg\n",
      "9 173e7c2ae39b17.jpg\n",
      "10 17859c46b3b92c.jpg\n",
      "11 1bf005bbaf7b94.jpg\n",
      "12 1c2b32fbd457a8.jpg\n",
      "13 1c98a48e5510ec.jpg\n",
      "14 1e0f47c2670231.jpg\n",
      "15 276d36e398c069.jpg\n",
      "16 29a05a0fd700ac.jpg\n",
      "17 2dea37f104896b.jpg\n",
      "18 2f30fad4bdb2c2.jpg\n",
      "19 310d6a3cfe1a4e.jpg\n",
      "20 32cd06546e924c.jpg\n",
      "21 33e729741fa101.jpg\n",
      "22 388839242d89f1.jpg\n",
      "23 412fbb13ecded7.jpg\n",
      "24 422be15adae561.jpg\n",
      "25 443badcd373167.jpg\n",
      "26 44616a41d8623a.jpg\n",
      "27 4596781034fe02.jpg\n",
      "28 45ef7ada1f32f2.jpg\n",
      "29 4684b764684b67.jpg\n",
      "30 488081370abe3b.jpg\n",
      "31 48cc1de8b1df31.jpg\n",
      "32 4def4bbec979e4.jpg\n",
      "33 50864141b27141.jpg\n",
      "34 51f6fe89078ccb.jpg\n",
      "35 5784688ed96534.jpg\n",
      "36 5ab8a4f5164040.jpg\n",
      "37 5c955e1d880f03.jpg\n",
      "38 5db729be093c0b.jpg\n",
      "39 5f2df3303a7ae1.jpg\n",
      "40 626895c87c1c41.jpg\n",
      "41 66a962c5a3605c.jpg\n",
      "42 6c46e7b095aa24.jpg\n",
      "43 6f9b4676d9a475.jpg\n",
      "44 7081e21b53ae0c.jpg\n",
      "45 72a5243e6c0a17.jpg\n",
      "46 72a6e6077a4698.jpg\n",
      "47 76959f74ef0650.jpg\n",
      "48 7b8f9fdb48aa42.jpg\n",
      "49 8028fb54baaa69.jpg\n",
      "50 8242a974d5e154.jpg\n",
      "51 861e78b09f1d7b.jpg\n",
      "52 873ae5681a8b4a.jpg\n",
      "53 89849f034f03e1.jpg\n",
      "54 8a2e638a356ab6.jpg\n",
      "55 8e5ce3b37f7e73.jpg\n",
      "56 954f75c293ff2e.jpg\n",
      "57 9b84c6371a8495.jpg\n",
      "58 a6385de6dc6abc.jpg\n",
      "59 a8291aa56f6894.jpg\n",
      "60 a9a0c044510f3f.jpg\n",
      "61 ab7f0a6c8b8e23.jpg\n",
      "62 afae935fef6f8a.jpg\n",
      "63 b56e8380f6defc.jpg\n",
      "64 b56f861d07340b.jpg\n",
      "65 b711aee5f18d86.jpg\n",
      "66 b792b3b1d8dbff.jpg\n",
      "67 b79aade18a298b.jpg\n",
      "68 b7f1e13098eb0b.jpg\n",
      "69 bae7b5b5126008.jpg\n",
      "70 bbb5750462cae0.jpg\n",
      "71 bc94aeccebf261.jpg\n",
      "72 bfabd5abd40a08.jpg\n",
      "73 c5ff98211b8933.jpg\n",
      "74 c602a84160ac56.jpg\n",
      "75 c91fbeae6a0a40.jpg\n",
      "76 c9ac0aa8a47e2a.jpg\n",
      "77 ccaa3c4b45d7c4.jpg\n",
      "78 d52f79c00711d5.jpg\n",
      "79 d86e9b2393119b.jpg\n",
      "80 d8e318aa5afe19.jpg\n",
      "81 dad1686015f9f5.jpg\n",
      "82 e40c66116b03ac.jpg\n",
      "83 e6310d8569ec9e.jpg\n",
      "84 e77e7e16519797.jpg\n",
      "85 f36f82bc154ef5.jpg\n",
      "86 f3b3bc05df3ea8.jpg\n",
      "87 f65a5529651e94.jpg\n",
      "88 f99ef9fb718218.jpg\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "hr_folder = 'data/train/HR/'\n",
    "lr_folder = 'data/train/LR/'\n",
    "os.makedirs(lr_folder, exist_ok=True)\n",
    "\n",
    "# Downsample HR images to create LR images\n",
    "count = 1\n",
    "for filename in os.listdir(hr_folder):\n",
    "    print(count, filename)\n",
    "    count+=1\n",
    "    if filename.endswith(\".png\") or filename.endswith(\".jpg\"):\n",
    "        hr_image_path = os.path.join(hr_folder, filename)\n",
    "        lr_image_path = os.path.join(lr_folder, filename)\n",
    "        downsample_image(hr_image_path, lr_image_path, scale_factor=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d9d31a",
   "metadata": {},
   "source": [
    "# SR Generator (GSR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3bd8a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.prelu = nn.PReLU()\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.conv2(self.prelu(self.conv1(x)))\n",
    "\n",
    "class GSR(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, num_residuals=5):\n",
    "        super(GSR, self).__init__()\n",
    "        self.encoder = nn.Conv2d(in_channels, 64, kernel_size=5, padding=2)\n",
    "        self.resnet = nn.Sequential(*[ResidualBlock(64) for _ in range(num_residuals)])\n",
    "        self.decoder = nn.Conv2d(64, out_channels, kernel_size=5, padding=2)\n",
    "        self.upsample = nn.Upsample(scale_factor=4, mode='bicubic')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.upsample(x)\n",
    "        x = self.encoder(x)\n",
    "        x = self.resnet(x)\n",
    "        x = self.decoder(x)\n",
    "        return torch.clamp(x, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd94aba",
   "metadata": {},
   "source": [
    "# Discriminators (Dx and Dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "554a00ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            self._block(64, 128, 3, 1),\n",
    "            self._block(128, 256, 3, 1),\n",
    "            self._block(256, 512, 3, 1),\n",
    "            nn.Conv2d(512, 1, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad59738d",
   "metadata": {},
   "source": [
    "# LR Generator (GLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc716051",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLR(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        super(GLR, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            self._block(64, 128, 3, 2),\n",
    "            self._block(128, 256, 3, 2),\n",
    "            nn.Conv2d(256, out_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5135d049",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d048bccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRResCycGANLosses:\n",
    "    def __init__(self, vgg):\n",
    "        self.vgg = vgg\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        self.tv_loss = TVLoss()\n",
    "\n",
    "    def perceptual_loss(self, sr, hr):\n",
    "        sr_vgg = self.vgg((sr - vgg_mean) / vgg_std)\n",
    "        hr_vgg = self.vgg((hr - vgg_mean) / vgg_std)\n",
    "        return self.l1_loss(sr_vgg, hr_vgg)\n",
    "\n",
    "    def gan_loss(self, sr_pred, hr_pred):\n",
    "        return F.binary_cross_entropy_with_logits(sr_pred, torch.ones_like(sr_pred)) + \\\n",
    "               F.binary_cross_entropy_with_logits(hr_pred, torch.zeros_like(hr_pred))\n",
    "\n",
    "    def total_variation_loss(self, sr):\n",
    "        return self.tv_loss(sr)\n",
    "\n",
    "    def content_loss(self, sr, hr):\n",
    "        return self.l1_loss(sr, hr)\n",
    "\n",
    "    def cyclic_loss(self, lr_recon, lr):\n",
    "        return self.l1_loss(lr_recon, lr)\n",
    "\n",
    "    def total_loss(self, sr, hr, lr_recon, lr, sr_pred, hr_pred):\n",
    "        l_per = self.perceptual_loss(sr, hr)\n",
    "        l_gan = self.gan_loss(sr_pred, hr_pred)\n",
    "        l_tv = self.total_variation_loss(sr)\n",
    "        l_content = self.content_loss(sr, hr)\n",
    "        l_cyclic = self.cyclic_loss(lr_recon, lr)\n",
    "        return l_per + l_gan + l_tv + 10 * l_content + 10 * l_cyclic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02d292d3-f107-40ea-a816-60a521671cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TVLoss(nn.Module):\n",
    "    def __init__(self, tv_loss_weight=1):\n",
    "        super(TVLoss, self).__init__()\n",
    "        self.tv_loss_weight = tv_loss_weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size()[0]\n",
    "        h_x = x.size()[2]\n",
    "        w_x = x.size()[3]\n",
    "        count_h = self._tensor_size(x[:, :, 1:, :])\n",
    "        count_w = self._tensor_size(x[:, :, :, 1:])\n",
    "        h_tv = torch.pow((x[:, :, 1:, :] - x[:, :, :h_x - 1, :]), 2).sum()\n",
    "        w_tv = torch.pow((x[:, :, :, 1:] - x[:, :, :, :w_x - 1]), 2).sum()\n",
    "        return self.tv_loss_weight * 2 * (h_tv / count_h + w_tv / count_w) / batch_size\n",
    "\n",
    "    def _tensor_size(self, t):\n",
    "        return t.size()[1] * t.size()[2] * t.size()[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a46108",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94837412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_srrescycgan(generator_sr, generator_lr, discriminator_hr, discriminator_lr, data_loader, optimizer_g, optimizer_d, losses, num_epochs, accumulation_steps=2):\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (lr, hr) in enumerate(data_loader):\n",
    "            lr, hr = lr.to(device), hr.to(device)\n",
    "\n",
    "            # Train Generators\n",
    "            optimizer_g.zero_grad()\n",
    "            with autocast():\n",
    "                sr = generator_sr(lr)\n",
    "                lr_recon = generator_lr(sr)\n",
    "                sr_pred = discriminator_hr(sr)\n",
    "                hr_pred = discriminator_hr(hr)\n",
    "                loss_g = losses.total_loss(sr, hr, lr_recon, lr, sr_pred, hr_pred) / accumulation_steps\n",
    "\n",
    "            scaler.scale(loss_g).backward()\n",
    "\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                scaler.step(optimizer_g)\n",
    "                scaler.update()\n",
    "                optimizer_g.zero_grad()\n",
    "\n",
    "            # Train Discriminators\n",
    "            optimizer_d.zero_grad()\n",
    "            with autocast():\n",
    "                sr_pred = discriminator_hr(sr.detach())\n",
    "                hr_pred = discriminator_hr(hr)\n",
    "                loss_d_hr = losses.gan_loss(sr_pred, hr_pred) / accumulation_steps\n",
    "\n",
    "                lr_pred = discriminator_lr(lr_recon.detach())\n",
    "                lr_real_pred = discriminator_lr(lr)\n",
    "                loss_d_lr = losses.gan_loss(lr_pred, lr_real_pred) / accumulation_steps\n",
    "\n",
    "                loss_d = loss_d_hr + loss_d_lr\n",
    "\n",
    "            scaler.scale(loss_d).backward()\n",
    "\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                scaler.step(optimizer_d)\n",
    "                scaler.update()\n",
    "                optimizer_d.zero_grad()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss G: {loss_g.item() * accumulation_steps}, Loss D: {loss_d.item() * accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae6d1c5",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46b670d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRDataset(Dataset):\n",
    "    def __init__(self, lr_dir, hr_dir, transform=None):\n",
    "        self.lr_dir = lr_dir\n",
    "        self.hr_dir = hr_dir\n",
    "        self.lr_images = sorted([file for file in os.listdir(lr_dir) if file.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "        self.hr_images = sorted([file for file in os.listdir(hr_dir) if file.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lr_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lr_image_path = os.path.join(self.lr_dir, self.lr_images[idx])\n",
    "        hr_image_path = os.path.join(self.hr_dir, self.hr_images[idx])\n",
    "        \n",
    "        lr_image = Image.open(lr_image_path).convert('RGB')\n",
    "        hr_image = Image.open(hr_image_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            lr_image = self.transform(lr_image)\n",
    "            hr_image = self.transform(hr_image)\n",
    "        \n",
    "        return lr_image, hr_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59e512f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 256, 256]) torch.Size([8, 3, 256, 256])\n",
      "torch.Size([8, 3, 256, 256]) torch.Size([8, 3, 256, 256])\n",
      "torch.Size([8, 3, 256, 256]) torch.Size([8, 3, 256, 256])\n",
      "torch.Size([8, 3, 256, 256]) torch.Size([8, 3, 256, 256])\n",
      "torch.Size([8, 3, 256, 256]) torch.Size([8, 3, 256, 256])\n",
      "torch.Size([8, 3, 256, 256]) torch.Size([8, 3, 256, 256])\n",
      "torch.Size([8, 3, 256, 256]) torch.Size([8, 3, 256, 256])\n",
      "torch.Size([8, 3, 256, 256]) torch.Size([8, 3, 256, 256])\n",
      "torch.Size([8, 3, 256, 256]) torch.Size([8, 3, 256, 256])\n",
      "torch.Size([8, 3, 256, 256]) torch.Size([8, 3, 256, 256])\n",
      "torch.Size([8, 3, 256, 256]) torch.Size([8, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "lr_dir = 'data/train/LR/'\n",
    "hr_dir = 'data/train/HR/'\n",
    "\n",
    "# Dataset and DataLoader\n",
    "# Define a transform to resize images to 256x256 and convert to tensor\n",
    "transform = Compose([\n",
    "    Resize((256, 256)),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "dataset = SRDataset(lr_dir, hr_dir, transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Example usage\n",
    "for lr, hr in data_loader:\n",
    "    print(lr.shape, hr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faea22ba-ac59-47f6-8a3d-9dfe3843952a",
   "metadata": {},
   "source": [
    "# Add VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8b46518-24a8-4c91-9c7f-af9eaa3ba576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define VGG19 model for perceptual loss\n",
    "class VGG19(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG19, self).__init__()\n",
    "        vgg = vgg19(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(vgg.features.children())[:36]).eval()  # Use first 36 layers\n",
    "\n",
    "        # Freeze parameters\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.features(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6420238f-0ed7-4ca3-ae20-1bed3b8725eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize input images for VGG19\n",
    "vgg_mean = torch.tensor([0.485, 0.456, 0.406]).cuda().view(1, 3, 1, 1)\n",
    "vgg_std = torch.tensor([0.229, 0.224, 0.225]).cuda().view(1, 3, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac7ed87",
   "metadata": {},
   "source": [
    "# Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df2b1302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benre\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\benre\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1024.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m losses \u001b[38;5;241m=\u001b[39m SRResCycGANLosses(vgg\u001b[38;5;241m=\u001b[39mvgg)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m train_srrescycgan(generator_sr, generator_lr, discriminator_hr, discriminator_lr, data_loader, optimizer_g, optimizer_d, losses, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 11\u001b[0m, in \u001b[0;36mtrain_srrescycgan\u001b[1;34m(generator_sr, generator_lr, discriminator_hr, discriminator_lr, data_loader, optimizer_g, optimizer_d, losses, num_epochs, accumulation_steps)\u001b[0m\n\u001b[0;32m      9\u001b[0m sr \u001b[38;5;241m=\u001b[39m generator_sr(lr)\n\u001b[0;32m     10\u001b[0m lr_recon \u001b[38;5;241m=\u001b[39m generator_lr(sr)\n\u001b[1;32m---> 11\u001b[0m sr_pred \u001b[38;5;241m=\u001b[39m discriminator_hr(sr)\n\u001b[0;32m     12\u001b[0m hr_pred \u001b[38;5;241m=\u001b[39m discriminator_hr(hr)\n\u001b[0;32m     13\u001b[0m loss_g \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mtotal_loss(sr, hr, lr_recon, lr, sr_pred, hr_pred) \u001b[38;5;241m/\u001b[39m accumulation_steps\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 21\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet(x)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:175\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    168\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    170\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;66;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;00m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_running_stats\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrack_running_stats \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    184\u001b[0m     bn_training,\n\u001b[0;32m    185\u001b[0m     exponential_average_factor,\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps,\n\u001b[0;32m    187\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:2509\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m   2507\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m-> 2509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[0;32m   2510\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39menabled\n\u001b[0;32m   2511\u001b[0m )\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "# Model, Optimizer, and Losses\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "generator_sr = GSR().to(device)\n",
    "generator_lr = GLR().to(device)\n",
    "discriminator_hr = Discriminator().to(device)\n",
    "discriminator_lr = Discriminator().to(device)\n",
    "vgg = VGG19().to(device)\n",
    "optimizer_g = torch.optim.Adam(list(generator_sr.parameters()) + list(generator_lr.parameters()), lr=1e-4, betas=(0.9, 0.999))\n",
    "optimizer_d = torch.optim.Adam(list(discriminator_hr.parameters()) + list(discriminator_lr.parameters()), lr=1e-4, betas=(0.9, 0.999))\n",
    "losses = SRResCycGANLosses(vgg=vgg)\n",
    "\n",
    "# Train the model\n",
    "train_srrescycgan(generator_sr, generator_lr, discriminator_hr, discriminator_lr, data_loader, optimizer_g, optimizer_d, losses, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1a7a877-3a1b-4149-bf4a-91597d862eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f890c43-426e-4a13-a40e-e08061c7dccf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
