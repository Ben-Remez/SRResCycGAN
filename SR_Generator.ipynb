{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Receive output from SR generator for a low resoultion image"
      ],
      "metadata": {
        "id": "gB4jQC1ms83k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "TIiuanGWtfSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision.transforms import ToTensor, ToPILImage, Normalize\n",
        "\n",
        "import lpips\n",
        "\n",
        "from skimage.metrics import structural_similarity as SSIM"
      ],
      "metadata": {
        "id": "g4zYSdo7tGZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SR Generator class"
      ],
      "metadata": {
        "id": "1hnMvpLSuK0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "        self.prelu = nn.PReLU()\n",
        "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.conv2(self.prelu(self.conv1(x)))\n",
        "\n",
        "class GSR(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=3, num_residuals=5):\n",
        "        super(GSR, self).__init__()\n",
        "        self.encoder = nn.Conv2d(in_channels, 64, kernel_size=5, padding=2)\n",
        "        self.resnet = nn.Sequential(*[ResidualBlock(64) for _ in range(num_residuals)])\n",
        "        self.decoder = nn.Conv2d(64, out_channels, kernel_size=5, padding=2)\n",
        "        self.upsample = nn.Upsample(scale_factor=4, mode='bicubic')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.upsample(x)\n",
        "        x = self.encoder(x)\n",
        "        x = self.resnet(x)\n",
        "        x = self.decoder(x)\n",
        "        return torch.clamp(x, 0, 1)"
      ],
      "metadata": {
        "id": "Km0I1Lg-uNb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the weights of the pre-trained model"
      ],
      "metadata": {
        "id": "I1DwRvS2tNWe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adGDt3G5s7gl",
        "outputId": "ec1d221b-c537-4416-eb3b-25121fab7ca7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GSR(\n",
              "  (encoder): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "  (resnet): Sequential(\n",
              "    (0): ResidualBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (prelu): PReLU(num_parameters=1)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (1): ResidualBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (prelu): PReLU(num_parameters=1)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (2): ResidualBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (prelu): PReLU(num_parameters=1)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (3): ResidualBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (prelu): PReLU(num_parameters=1)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "    (4): ResidualBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (prelu): PReLU(num_parameters=1)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (decoder): Conv2d(64, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "  (upsample): Upsample(scale_factor=4.0, mode='bicubic')\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Model, Optimizer, and Losses\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Assume GSR is the class definition for the SR generator\n",
        "generator_sr = GSR().to(device)  # Make sure the device is set (e.g., 'cuda' or 'cpu')\n",
        "\n",
        "# Load the pre-trained weights\n",
        "generator_sr.load_state_dict(torch.load('generator_sr.pth'))\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "generator_sr.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the low-resolution image"
      ],
      "metadata": {
        "id": "rNPYgonLtUV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the low-resolution image\n",
        "lr_image = Image.open('input/LR/8e5ce3b37f7e73.jpg').convert('RGB')\n",
        "\n",
        "# Preprocess the image: Convert to tensor and add a batch dimension\n",
        "transform = ToTensor()\n",
        "lr_image_tensor = transform(lr_image).unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
        "\n",
        "# Normalize the input image before passing it through the generator\n",
        "normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "lr_image_tensor = normalize(lr_image_tensor)"
      ],
      "metadata": {
        "id": "UZjZPdBvtI7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate the high-resolution image"
      ],
      "metadata": {
        "id": "_O6j5cT7ttYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the high-resolution image\n",
        "with torch.no_grad():  # Disable gradient calculation for inference\n",
        "    output = generator_sr(lr_image_tensor)\n",
        "\n",
        "# Denormalize the output before converting to an image\n",
        "output = output * torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1).to(device) + \\\n",
        "         torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).to(device)\n",
        "\n",
        "# Remove the batch dimension and convert back to a PIL image\n",
        "sr_image_tensor = output.squeeze(0)  # Remove batch dimension\n",
        "to_pil = ToPILImage()\n",
        "sr_image = to_pil(sr_image_tensor.cpu())  # Move to CPU and convert to PIL image"
      ],
      "metadata": {
        "id": "jBHYgjZZtw66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save high resolution image output"
      ],
      "metadata": {
        "id": "rnz5c4bzt0NL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the high-resolution image\n",
        "sr_image.save('output/HR/super_resolved_image.jpg')"
      ],
      "metadata": {
        "id": "OjSCTwmht5xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics"
      ],
      "metadata": {
        "id": "wooshipMwt93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_psnr(img1, img2):\n",
        "    mse = F.mse_loss(img1, img2)\n",
        "    if mse == 0:\n",
        "        return float('inf')\n",
        "    psnr = 20 * torch.log10(1.0 / torch.sqrt(mse))\n",
        "    return psnr.item()"
      ],
      "metadata": {
        "id": "wcRUFb3be1ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_ssim(img1, img2, win_size=7):\n",
        "    img1_np = img1.squeeze(0).permute(1, 2, 0).cpu().numpy()  # Convert to HWC format and NumPy array\n",
        "    img2_np = img2.squeeze(0).permute(1, 2, 0).cpu().numpy()  # Convert to HWC format and NumPy array\n",
        "    ssim_value = SSIM(img1_np, img2_np, multichannel=True, data_range=img1_np.max() - img1_np.min(), win_size=win_size)\n",
        "    return ssim_value"
      ],
      "metadata": {
        "id": "2tu903GvfpkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_lpips(img1, img2):\n",
        "    # Initialize LPIPS model\n",
        "    loss_fn = lpips.LPIPS(net='alex')\n",
        "\n",
        "    # Ensure img1 and img2 are normalized and have the same dimensions\n",
        "    lpips_value = loss_fn(img1, img2)\n",
        "    return lpips_value.item()"
      ],
      "metadata": {
        "id": "IL1dFDw1fq_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load images and run calaculation of metrics for the given images"
      ],
      "metadata": {
        "id": "9RGEn2aiwgz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sr_image = Image.open('output/HR/super_resolved_image.png').convert('RGB')\n",
        "hr_image = Image.open('input/HR/8e5ce3b37f7e73.jpg').convert('RGB')\n",
        "\n",
        "# Convert to tensors (this will normalize the images to the range [0, 1])\n",
        "sr_image_tensor = ToTensor()(sr_image)  # Shape will be [3, H, W]\n",
        "hr_image_tensor = ToTensor()(hr_image)  # Shape will be [3, H, W]\n",
        "\n",
        "# Add a batch dimension (shape will be [1, 3, H, W])\n",
        "sr_image_tensor = sr_image_tensor.unsqueeze(0)\n",
        "hr_image_tensor = hr_image_tensor.unsqueeze(0)\n",
        "\n",
        "# Resize sr_image_tensor to match the dimensions of hr_image_tensor\n",
        "sr_image_tensor_resized = F.interpolate(sr_image_tensor, size=hr_image_tensor.shape[2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "# Normalize to [-1, 1] for LPIPS\n",
        "sr_image_tensor_lpips = 2 * sr_image_tensor - 1\n",
        "hr_image_tensor_lpips = 2 * hr_image_tensor - 1\n",
        "\n",
        "sr_image_tensor_lpips = F.interpolate(sr_image_tensor_lpips, size=hr_image_tensor_lpips.shape[2:], mode='bilinear', align_corners=False)"
      ],
      "metadata": {
        "id": "A5-zqUbRgt-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PSNR\n",
        "psnr_value = calculate_psnr(sr_image_tensor_resized, hr_image_tensor)\n",
        "print(f\"PSNR: {psnr_value} dB\")\n",
        "\n",
        "# SSIM\n",
        "ssim_value = calculate_ssim(sr_image_tensor_resized, hr_image_tensor, win_size=3)\n",
        "print(f\"SSIM: {ssim_value}\")\n",
        "\n",
        "# LPIPS\n",
        "lpips_value = calculate_lpips(sr_image_tensor_lpips, hr_image_tensor_lpips)\n",
        "print(f\"LPIPS: {lpips_value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sP8cawBShBcf",
        "outputId": "666bf508-2b70-4baf-da7d-70c1f04f73a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PSNR: 9.65070629119873 dB\n",
            "SSIM: 0.04575595462125225\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.10/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "LPIPS: 0.804252028465271\n"
          ]
        }
      ]
    }
  ]
}